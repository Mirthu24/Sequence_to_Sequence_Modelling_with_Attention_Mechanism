# **SEQUENCE TO SEQUENCE MODELLING WITH ATTENTION MECHANISM**

This project implements a Sequence-to-Sequence (Seq2Seq) model enhanced with an Attention Mechanism using PyTorch. The task involves reversing an input sequence of integers using deep learning, while visualizing attention weights to enhance interpretability.

### **PROJECT OVERVIEW**
1. Implemented a GRU-based Encoder–Decoder architecture.
2. Integrated Attention Mechanism for improved performance on sequence prediction.
3. Used a synthetic dataset where each output is the reversed input.
4. Evaluated model on training and unseen test data.
5. Visualized attention weights using heatmaps.

### **SKILLS GAINED**
1. Sequence-to-Sequence modeling with Attention
2. PyTorch-based deep learning model building
3. Model evaluation and visualization
4. Working with synthetic data for NLP tasks

### **DATSET**
1. **Type:** Synthetic integer sequences
2. **Source:** Randomly generated
3. **Input:** Integer sequence of length 30
4. **Output:** Reversed input sequence
5. **Vocabulary size:** 20
6. **Dataset size:** 1,00,000 samples for training + test samples

### **TECHNOLOGIES USED**
1. Python
2. PyTorch
3. NumPy
4. Matplotlib / Seaborn (for visualizations)
5. Google Colab (for training with GPU)

### **RESULTS**
• Achieved high accuracy on both training and unseen test data.

• Loss curve showed consistent convergence.

• Attention heatmaps clearly showed correct focus alignment.

• Predictions closely matched reversed sequences.

![download](https://github.com/user-attachments/assets/0dc22451-0ad1-4540-ad39-ff7dc16ce589)

### **FILES**
1. [**Code File**](https://colab.research.google.com/drive/16B0ENnZmGdsEb03RLg6VPYwK1FtIayyo?usp=sharing)
2. [**Documentation**](https://docs.google.com/document/d/1Rh42NoNUdobiWCB2_Vj_NkeST-j9urb3/edit?usp=sharing&ouid=110283387145117356226&rtpof=true&sd=true)
3. [**Saved model**](https://drive.google.com/file/d/1-O_TGnul6RSCTGgWJGsiMKpw0Y55lt5F/view?usp=sharing)



